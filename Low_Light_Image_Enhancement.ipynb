{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxzs6SFHkhWJ",
        "outputId": "6cffa327-a238-492d-fe7d-9763a20eacbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/611.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/611.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow tensorflow_addons --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARHa4TtCkh4S",
        "outputId": "a134a474-5c4c-4226-cf90-92d9e021fff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade --quiet\n",
        "!pip install git+https://github.com/wandb/wandb.git\n",
        "!yes | apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['WANDB_API_KEY'] = 'd01b92fa0118a7bba3d1f90cfda3e30f3addf78f'\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "from transformers.tf_utils import shape_list\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfoJo-p7w0TG",
        "outputId": "21320a07-b18b-43e1-fcd4-447c55948b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/wandb/wandb.git\n",
            "  Cloning https://github.com/wandb/wandb.git to /tmp/pip-req-build-0tr5xubf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/wandb/wandb.git /tmp/pip-req-build-0tr5xubf\n",
            "  Resolved https://github.com/wandb/wandb.git to commit 6b8a4dbde1e120784527ab60b8985be73d8383da\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.16.5.dev1)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb==0.16.5.dev1)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb==0.16.5.dev1)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (6.0.1)\n",
            "Collecting setproctitle (from wandb==0.16.5.dev1)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.16.5.dev1) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.16.5.dev1) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.16.5.dev1)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.16.5.dev1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.16.5.dev1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.16.5.dev1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.16.5.dev1) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.16.5.dev1)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: wandb\n",
            "  Building wheel for wandb (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wandb: filename=wandb-0.16.5.dev1-py3-none-any.whl size=2202783 sha256=57b35ff42df787fee82c123daa5133105b2fc0c592e6210f8c1903b63f5d8777\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i2co2iq9/wheels/d4/e8/45/c3dcab816be8169b14b0f8f9da0813be53e067c27980153a1f\n",
            "Successfully built wandb\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.5.dev1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package libcudnn8 is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "\n",
            "\u001b[1;31mE: \u001b[0mVersion '8.1.0.77-1+cuda11.2' for 'libcudnn8' was not found\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"mirnet-v2\",entity=\"nemesis_xvi\", job_type=\"colab-train\")\n",
        "\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "config.seed = 42\n",
        "random.seed(config.seed)\n",
        "tf.random.set_seed(config.seed)\n",
        "\n",
        "config.num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "config.dataset_artifact_address = 'nemesis_xvi/mirnet-v2/lol-dataset:v0'\n",
        "config.image_size = 128\n",
        "config.max_train_images = 400\n",
        "config.batch_size_per_replica = 4\n",
        "config.batch_size = config.batch_size_per_replica * strategy.num_replicas_in_sync\n",
        "\n",
        "config.channels = 80\n",
        "config.channel_factor = 1.5\n",
        "config.num_mrb_blocks = 2\n",
        "config.add_residual_connection = True\n",
        "\n",
        "config.initial_learning_rate = 2e-4\n",
        "config.minimum_learning_rate = 1e-6\n",
        "config.epochs = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "pOutp-Uckjm3",
        "outputId": "795a35db-08a8-45bf-8ff6-5439254fc410",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchesslover-201\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem at: <ipython-input-4-99e6113214e5> 1 <cell line: 1>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CommError",
          "evalue": "It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 403: Forbidden)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-99e6113214e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mirnet-v2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nemesis_xvi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"colab-train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    784\u001b[0m                 \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mrun_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# for mypy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCommError\u001b[0m: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 403: Forbidden)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir LOLdataset"
      ],
      "metadata": {
        "id": "YM-VFQj3ktOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/LOLdataset.zip' -d '/content/LOLdataset'"
      ],
      "metadata": {
        "id": "ORxvHdW_kmzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/LOLdataset\"\n",
        "TRAIN_LOW_LIGHT_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/low/*\"))\n",
        ")[:config.max_train_images]\n",
        "TRAIN_GROUND_TRUTH_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/high/*\"))\n",
        ")[:config.max_train_images]\n",
        "\n",
        "VAL_LOW_LIGHT_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/low/*\"))\n",
        ")[config.max_train_images:]\n",
        "VAL_GROUND_TRUTH_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"our485/high/*\"))\n",
        ")[config.max_train_images:]\n",
        "\n",
        "TEST_LOW_LIGHT_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"eval15/low/*\"))\n",
        ")\n",
        "TEST_GROUND_TRUTH_IMAGES = sorted(\n",
        "    glob(os.path.join(dataset_dir, \"eval15/high/*\"))\n",
        ")\n",
        "\n",
        "print(f\"Train Dataset: ({len(TRAIN_LOW_LIGHT_IMAGES)}, {len(TRAIN_GROUND_TRUTH_IMAGES)})\")\n",
        "print(f\"Validation Dataset: ({len(VAL_LOW_LIGHT_IMAGES)}, {len(VAL_GROUND_TRUTH_IMAGES)})\")\n",
        "print(f\"Test Dataset: ({len(TEST_LOW_LIGHT_IMAGES)}, {len(TEST_GROUND_TRUTH_IMAGES)})\")"
      ],
      "metadata": {
        "id": "RZ_9LtAnk4ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wGoDY4U-k4wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2022-12-06T02:49:58.845845Z",
          "iopub.status.busy": "2022-12-06T02:49:58.845486Z",
          "iopub.status.idle": "2022-12-06T02:49:58.864188Z",
          "shell.execute_reply": "2022-12-06T02:49:58.863272Z"
        },
        "papermill": {
          "duration": 0.061143,
          "end_time": "2022-12-06T02:49:58.866882",
          "exception": false,
          "start_time": "2022-12-06T02:49:58.805739",
          "status": "completed"
        },
        "tags": [],
        "id": "9b4580f9"
      },
      "outputs": [],
      "source": [
        "def read_image(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.cast(image, dtype=tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def random_crop(low_image, gt_image):\n",
        "    low_image_shape = tf.shape(low_image)[:2]\n",
        "    crop_width = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[1] - config.image_size + 1, dtype=tf.int32\n",
        "    )\n",
        "    crop_height = tf.random.uniform(\n",
        "        shape=(), maxval=low_image_shape[0] - config.image_size + 1, dtype=tf.int32\n",
        "    )\n",
        "    low_image_cropped = low_image[\n",
        "        crop_height : crop_height + config.image_size,\n",
        "        crop_width : crop_width + config.image_size\n",
        "    ]\n",
        "    gt_image_cropped = gt_image[\n",
        "        crop_height : crop_height + config.image_size,\n",
        "        crop_width : crop_width + config.image_size\n",
        "    ]\n",
        "    low_image_cropped.set_shape([config.image_size, config.image_size, 3])\n",
        "    gt_image_cropped.set_shape([config.image_size, config.image_size, 3])\n",
        "    return low_image_cropped, gt_image_cropped\n",
        "\n",
        "\n",
        "def resize_images(low_image, gt_image):\n",
        "    low_image = tf.image.resize(low_image, size=[config.image_size, config.image_size])\n",
        "    gt_image = tf.image.resize(gt_image, size=[config.image_size, config.image_size])\n",
        "    low_image.set_shape([config.image_size, config.image_size, 3])\n",
        "    gt_image.set_shape([config.image_size, config.image_size, 3])\n",
        "    return low_image, gt_image\n",
        "\n",
        "\n",
        "def load_data(low_light_image_path, enhanced_image_path, apply_resize):\n",
        "    low_light_image = read_image(low_light_image_path)\n",
        "    enhanced_image = read_image(enhanced_image_path)\n",
        "    low_light_image, enhanced_image = (\n",
        "        resize_images(low_light_image, enhanced_image) if apply_resize\n",
        "        else random_crop(low_light_image, enhanced_image)\n",
        "    )\n",
        "    return low_light_image, enhanced_image\n",
        "\n",
        "\n",
        "def get_dataset(low_light_images, enhanced_images, apply_resize):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))\n",
        "    dataset = dataset.map(\n",
        "        partial(load_data, apply_resize=apply_resize),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    dataset = dataset.batch(config.batch_size, drop_remainder=True)\n",
        "    dataset = dataset.prefetch(AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = get_dataset(TRAIN_LOW_LIGHT_IMAGES, TRAIN_GROUND_TRUTH_IMAGES, apply_resize=False)\n",
        "val_dataset = get_dataset(VAL_LOW_LIGHT_IMAGES, VAL_GROUND_TRUTH_IMAGES, apply_resize=True)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset.element_spec)\n",
        "print(\"Validation Dataset:\", val_dataset.element_spec)"
      ],
      "metadata": {
        "id": "e46CF-zhk9CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LMU0arxk_cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:49:59.965376Z",
          "iopub.status.busy": "2022-12-06T02:49:59.964942Z",
          "iopub.status.idle": "2022-12-06T02:49:59.977824Z",
          "shell.execute_reply": "2022-12-06T02:49:59.976931Z"
        },
        "papermill": {
          "duration": 0.063307,
          "end_time": "2022-12-06T02:49:59.980331",
          "exception": false,
          "start_time": "2022-12-06T02:49:59.917024",
          "status": "completed"
        },
        "tags": [],
        "id": "dc7c8399"
      },
      "outputs": [],
      "source": [
        "class SelectiveKernelFeatureFusion(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, *args, **kwargs) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.hidden_channels = max(int(channels / 8), 4)\n",
        "\n",
        "        self.average_pooling = tfa.layers.AdaptiveAveragePooling2D(output_size=1)\n",
        "\n",
        "        self.conv_channel_downscale = tf.keras.layers.Conv2D(\n",
        "            self.hidden_channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "        self.conv_attention_1 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, strides=1, padding=\"same\"\n",
        "        )\n",
        "        self.conv_attention_2 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, strides=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        combined_input_features = inputs[0] + inputs[1]\n",
        "        channel_wise_statistics = self.average_pooling(combined_input_features)\n",
        "        downscaled_channel_wise_statistics = self.conv_channel_downscale(\n",
        "            channel_wise_statistics\n",
        "        )\n",
        "        attention_vector_1 = self.softmax(\n",
        "            self.conv_attention_1(downscaled_channel_wise_statistics)\n",
        "        )\n",
        "        attention_vector_2 = self.softmax(\n",
        "            self.conv_attention_2(downscaled_channel_wise_statistics)\n",
        "        )\n",
        "        selected_features = (\n",
        "            inputs[0] * attention_vector_1 + inputs[1] * attention_vector_2\n",
        "        )\n",
        "        return selected_features"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4pCWwJGGlCI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.142636Z",
          "iopub.status.busy": "2022-12-06T02:50:00.142183Z",
          "iopub.status.idle": "2022-12-06T02:50:00.161579Z",
          "shell.execute_reply": "2022-12-06T02:50:00.160675Z"
        },
        "papermill": {
          "duration": 0.061416,
          "end_time": "2022-12-06T02:50:00.163922",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.102506",
          "status": "completed"
        },
        "tags": [],
        "id": "4d2d79c7"
      },
      "outputs": [],
      "source": [
        "class ContextBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.mask_conv = tf.keras.layers.Conv2D(\n",
        "            1, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.channel_add_conv_1 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "        self.channel_add_conv_2 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=1)\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "    def modeling(self, inputs):\n",
        "        _, height, width, channels = shape_list(inputs)\n",
        "        reshaped_inputs = tf.expand_dims(\n",
        "            tf.reshape(inputs, (-1, channels, height * width)), axis=1\n",
        "        )\n",
        "\n",
        "        context_mask = self.mask_conv(inputs)\n",
        "        context_mask = tf.reshape(context_mask, (-1, height * width, 1))\n",
        "        context_mask = self.softmax(context_mask)\n",
        "        context_mask = tf.expand_dims(context_mask, axis=1)\n",
        "\n",
        "        context = tf.reshape(\n",
        "            tf.matmul(reshaped_inputs, context_mask), (-1, 1, 1, channels)\n",
        "        )\n",
        "        return context\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        context = self.modeling(inputs)\n",
        "        channel_add_term = self.channel_add_conv_1(context)\n",
        "        channel_add_term = self.leaky_relu(channel_add_term)\n",
        "        channel_add_term = self.channel_add_conv_2(channel_add_term)\n",
        "        return inputs + channel_add_term\n",
        "\n",
        "\n",
        "class ResidualContextBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, groups: int, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=3, padding=\"same\", groups=groups\n",
        "        )\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=3, padding=\"same\", groups=groups\n",
        "        )\n",
        "        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "        self.context_block = ContextBlock(channels=channels)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.context_block(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = x + inputs\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WvDhgYRlEhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.329594Z",
          "iopub.status.busy": "2022-12-06T02:50:00.329121Z",
          "iopub.status.idle": "2022-12-06T02:50:00.343934Z",
          "shell.execute_reply": "2022-12-06T02:50:00.343125Z"
        },
        "papermill": {
          "duration": 0.057854,
          "end_time": "2022-12-06T02:50:00.346565",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.288711",
          "status": "completed"
        },
        "tags": [],
        "id": "77cfe326"
      },
      "outputs": [],
      "source": [
        "class DownBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, channels: int, channel_factor: float, *args, **kwargs\n",
        "    ):\n",
        "        super(DownBlock, self).__init__(*args, **kwargs)\n",
        "        self.average_pool = tf.keras.layers.AveragePooling2D(\n",
        "            pool_size=2, strides=2\n",
        "        )\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            int(channels * channel_factor),\n",
        "            kernel_size=1,\n",
        "            strides=1,\n",
        "            padding=\"same\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.conv(self.average_pool(inputs))\n",
        "\n",
        "\n",
        "class DownSampleBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        scale_factor: int,\n",
        "        channel_factor: float,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(DownSampleBlock, self).__init__(*args, **kwargs)\n",
        "        self.layers = []\n",
        "        for _ in range(int(np.log2(scale_factor))):\n",
        "            self.layers.append(DownBlock(channels, channel_factor))\n",
        "            channels = int(channels * channel_factor)\n",
        "\n",
        "    def call(self, x, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, channel_factor: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.conv = tf.keras.layers.Conv2D(\n",
        "            int(channels // channel_factor), kernel_size=1, strides=1, padding=\"same\"\n",
        "        )\n",
        "        self.upsample = tf.keras.layers.UpSampling2D(size=2, interpolation=\"bilinear\")\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        return self.upsample(self.conv(inputs))\n",
        "\n",
        "\n",
        "class UpSampleBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self, channels: int, scale_factor: int, channel_factor: float, *args, **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.layers = []\n",
        "        for _ in range(int(np.log2(scale_factor))):\n",
        "            self.layers.append(UpBlock(channels, channel_factor))\n",
        "            channels = int(channels // channel_factor)\n",
        "\n",
        "    def call(self, x, *args, **kwargs):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HWQzvMuilHHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xbxlGHFlMm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.523405Z",
          "iopub.status.busy": "2022-12-06T02:50:00.522832Z",
          "iopub.status.idle": "2022-12-06T02:50:00.559399Z",
          "shell.execute_reply": "2022-12-06T02:50:00.558365Z"
        },
        "papermill": {
          "duration": 0.077697,
          "end_time": "2022-12-06T02:50:00.562014",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.484317",
          "status": "completed"
        },
        "tags": [],
        "id": "3551a758"
      },
      "outputs": [],
      "source": [
        "class MultiScaleResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        channel_factor: float,\n",
        "        groups: int,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # Residual Context Blocks\n",
        "        self.rcb_top = ResidualContextBlock(\n",
        "            int(channels * channel_factor**0), groups=groups\n",
        "        )\n",
        "        self.rcb_middle = ResidualContextBlock(\n",
        "            int(channels * channel_factor**1), groups=groups\n",
        "        )\n",
        "        self.rcb_bottom = ResidualContextBlock(\n",
        "            int(channels * channel_factor**2), groups=groups\n",
        "        )\n",
        "\n",
        "        # Downsample Blocks\n",
        "        self.down_2 = DownSampleBlock(\n",
        "            channels=int((channel_factor**0) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.down_4_1 = DownSampleBlock(\n",
        "            channels=int((channel_factor ** 0) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.down_4_2 = DownSampleBlock(\n",
        "            channels=int((channel_factor ** 1) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "\n",
        "        # UpSample Blocks\n",
        "        self.up21_1 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 1) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.up21_2 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 1) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.up32_1 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 2) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "        self.up32_2 = UpSampleBlock(\n",
        "            channels=int((channel_factor ** 2) * channels),\n",
        "            scale_factor=2,\n",
        "            channel_factor=channel_factor,\n",
        "        )\n",
        "\n",
        "        # SKFF Blocks\n",
        "        self.skff_top = SelectiveKernelFeatureFusion(\n",
        "            channels=int(channels * channel_factor**0)\n",
        "        )\n",
        "        self.skff_middle = SelectiveKernelFeatureFusion(\n",
        "            channels=int(channels * channel_factor**1)\n",
        "        )\n",
        "\n",
        "        # Convolution\n",
        "        self.conv_out = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=1, padding=\"same\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        x_top = inputs\n",
        "        x_middle = self.down_2(x_top)\n",
        "        x_bottom = self.down_4_2(self.down_4_1(x_top))\n",
        "\n",
        "        x_top = self.rcb_top(x_top)\n",
        "        x_middle = self.rcb_middle(x_middle)\n",
        "        x_bottom = self.rcb_bottom(x_bottom)\n",
        "\n",
        "        x_middle = self.skff_middle([x_middle, self.up32_1(x_bottom)])\n",
        "        x_top = self.skff_top([x_top, self.up21_1(x_middle)])\n",
        "\n",
        "        x_top = self.rcb_top(x_top)\n",
        "        x_middle = self.rcb_middle(x_middle)\n",
        "        x_bottom = self.rcb_bottom(x_bottom)\n",
        "\n",
        "        x_middle = self.skff_middle([x_middle, self.up32_2(x_bottom)])\n",
        "        x_top = self.skff_top([x_top, self.up21_2(x_middle)])\n",
        "\n",
        "        output = self.conv_out(x_top)\n",
        "        output = output + inputs\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nzc7opY8lOb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.731837Z",
          "iopub.status.busy": "2022-12-06T02:50:00.731425Z",
          "iopub.status.idle": "2022-12-06T02:50:00.748373Z",
          "shell.execute_reply": "2022-12-06T02:50:00.747468Z"
        },
        "papermill": {
          "duration": 0.063879,
          "end_time": "2022-12-06T02:50:00.750942",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.687063",
          "status": "completed"
        },
        "tags": [],
        "id": "d4d839d4"
      },
      "outputs": [],
      "source": [
        "class RecursiveResidualGroup(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        num_mrb_blocks: int,\n",
        "        channel_factor: float,\n",
        "        groups: int,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.layers = [\n",
        "            MultiScaleResidualBlock(channels, channel_factor, groups)\n",
        "            for _ in range(num_mrb_blocks)\n",
        "        ]\n",
        "        self.layers.append(\n",
        "            tf.keras.layers.Conv2D(\n",
        "                channels, kernel_size=3, strides=1, padding=\"same\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        residual = inputs\n",
        "        for layer in self.layers:\n",
        "            residual = layer(residual)\n",
        "        residual = residual + inputs\n",
        "        return residual\n",
        "\n",
        "\n",
        "class MirNetv2(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        channels: int,\n",
        "        channel_factor: float,\n",
        "        num_mrb_blocks: int,\n",
        "        add_residual_connection: bool,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.add_residual_connection = add_residual_connection\n",
        "\n",
        "        self.conv_in = tf.keras.layers.Conv2D(\n",
        "            channels, kernel_size=3, padding=\"same\"\n",
        "        )\n",
        "\n",
        "        self.rrg_block_1 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=1\n",
        "        )\n",
        "        self.rrg_block_2 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=2\n",
        "        )\n",
        "        self.rrg_block_3 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=4\n",
        "        )\n",
        "        self.rrg_block_4 = RecursiveResidualGroup(\n",
        "            channels, num_mrb_blocks, channel_factor, groups=4\n",
        "        )\n",
        "\n",
        "        self.conv_out = tf.keras.layers.Conv2D(\n",
        "            3, kernel_size=3, padding=\"same\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=None, mask=None):\n",
        "        shallow_features = self.conv_in(inputs)\n",
        "        deep_features = self.rrg_block_1(shallow_features)\n",
        "        deep_features = self.rrg_block_2(deep_features)\n",
        "        deep_features = self.rrg_block_3(deep_features)\n",
        "        deep_features = self.rrg_block_4(deep_features)\n",
        "        output = self.conv_out(deep_features)\n",
        "        output = output + inputs if self.add_residual_connection else output\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wIqlJLQKlQwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:00.920558Z",
          "iopub.status.busy": "2022-12-06T02:50:00.920081Z",
          "iopub.status.idle": "2022-12-06T02:50:00.937274Z",
          "shell.execute_reply": "2022-12-06T02:50:00.936325Z"
        },
        "papermill": {
          "duration": 0.06319,
          "end_time": "2022-12-06T02:50:00.939867",
          "exception": false,
          "start_time": "2022-12-06T02:50:00.876677",
          "status": "completed"
        },
        "tags": [],
        "id": "5c0df6bd"
      },
      "outputs": [],
      "source": [
        "class CharbonnierLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, epsilon: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.epsilon = tf.convert_to_tensor(epsilon)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        squared_difference = tf.square(y_true - y_pred)\n",
        "        return tf.reduce_mean(\n",
        "            tf.sqrt(squared_difference + tf.square(self.epsilon))\n",
        "        )\n",
        "\n",
        "\n",
        "class PSNRMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, max_val: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.max_val = max_val\n",
        "        self.psnr = tf.keras.metrics.Mean(name=\"psnr\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, *args, **kwargs):\n",
        "        psnr = tf.image.psnr(y_true, y_pred, max_val=self.max_val)\n",
        "        self.psnr.update_state(psnr, *args, **kwargs)\n",
        "\n",
        "    def result(self):\n",
        "        return self.psnr.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.psnr.reset_state()\n",
        "\n",
        "\n",
        "class SSIMMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, max_val: float, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.max_val = max_val\n",
        "        self.ssim = tf.keras.metrics.Mean(name=\"ssim\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, *args, **kwargs):\n",
        "        ssim = tf.image.ssim(y_true, y_pred, max_val=self.max_val)\n",
        "        self.ssim.update_state(ssim, *args, **kwargs)\n",
        "\n",
        "    def result(self):\n",
        "        return self.ssim.result()\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.ssim.reset_state()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ouGhAGPlS7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-12-06T02:50:01.105568Z",
          "iopub.status.busy": "2022-12-06T02:50:01.105142Z",
          "iopub.status.idle": "2022-12-06T02:50:15.880090Z",
          "shell.execute_reply": "2022-12-06T02:50:15.879079Z"
        },
        "papermill": {
          "duration": 14.819386,
          "end_time": "2022-12-06T02:50:15.882915",
          "exception": false,
          "start_time": "2022-12-06T02:50:01.063529",
          "status": "completed"
        },
        "tags": [],
        "id": "2a066f63"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model = MirNetv2(\n",
        "        channels=config.channels,\n",
        "        channel_factor=config.channel_factor,\n",
        "        num_mrb_blocks=config.num_mrb_blocks,\n",
        "        add_residual_connection=config.add_residual_connection\n",
        "    )\n",
        "\n",
        "    dummy_inputs = tf.ones((1, config.image_size, config.image_size, 3))\n",
        "    dummy_outputs = model(dummy_inputs)\n",
        "    model.summary(expand_nested=True)\n",
        "    print(\"\\nInput Shape:\", dummy_inputs.shape)\n",
        "    print(\"Output Shape:\", dummy_outputs.shape)\n",
        "\n",
        "    loss = CharbonnierLoss(epsilon=1e-3)\n",
        "\n",
        "    psnr_metric = PSNRMetric(max_val=1.0)\n",
        "    ssim_metric = SSIMMetric(max_val=1.0)\n",
        "\n",
        "    decay_steps = (config.max_train_images // config.batch_size) * config.epochs\n",
        "    lr_schedule_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=config.initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        alpha=config.minimum_learning_rate,\n",
        "    )\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        learning_rate=lr_schedule_fn, beta_1=0.9, beta_2=0.999,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "callbacks = [\n",
        "    WandbMetricsLogger(),\n",
        "    WandbModelCheckpoint(filepath=\"model\", save_best_only=False)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=config.epochs,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "v7dxkqbGlU0T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}